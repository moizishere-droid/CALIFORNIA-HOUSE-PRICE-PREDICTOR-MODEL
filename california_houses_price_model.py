# -*- coding: utf-8 -*-
"""CALIFORNIA HOUSES PRICE MODEL.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1u9auCWUk81wEGUB6bir8ZlkxwBxSxu_O

# ***IMPORTING LIBRARIES***
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

"""# ***Get the Dataset***"""

from sklearn.datasets import fetch_california_housing

# Load dataset
housing = fetch_california_housing(as_frame=True)
df = pd.DataFrame(housing.frame)
demo_df = df
df.head()

"""# ***EDA ON DATASET***"""

demo_df.head(2)

demo_df.info()

demo_df.describe() # WE NEED SCALING

# Correlation heatmap
plt.figure(figsize=(10,6))
sns.heatmap(df.corr(), annot=True, cmap="coolwarm",linewidths=0.5,linecolor="black")
plt.show()

# Price vs Latitude/Longitude
plt.figure(figsize=(10,6))
sns.scatterplot(data=demo_df, x="Longitude", y="Latitude", hue="MedHouseVal", palette="viridis", alpha=0.5)
plt.show()

"""# ***DATA PREPROCESSING***"""

from sklearn.model_selection import train_test_split

X = df.drop("MedHouseVal", axis=1)
y = df["MedHouseVal"]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

"""# ***CHECKING BEST MODEL TO BE***"""

!pip install lazypredict

from lazypredict.Supervised import LazyRegressor
clf = LazyRegressor(verbose=0,ignore_warnings=True)
models,predictions = clf.fit(X_train,X_test,y_train,y_test)
print(models)

"""## ***TRAINING MODEL***"""

!pip install optuna

"""# ***MODEL 1***"""

from lightgbm import LGBMRegressor
from sklearn.model_selection import cross_val_score, KFold

# MODEL NO 1 --> Mean R2: 0.611936021767489 Std: 0.00726304480387652
model_1 = LGBMRegressor(n_estimators=100, learning_rate=0.01, random_state=42)
model_1.fit(X_train, y_train)
y_pred = model_1.predict(X_test)


kf = KFold(n_splits=5, shuffle=True, random_state=42)
scores = cross_val_score(model_1, X, y, cv=kf, scoring='r2',verbose=0)
print("Mean R2:", scores.mean(), "Std:", scores.std())

# IMPROVE VERSION

from lightgbm import LGBMRegressor
from sklearn.model_selection import cross_val_score, KFold

params = {
    'num_leaves': 2790,
    'max_depth': 15,
    'learning_rate': 0.04134647593299318,
    'n_estimators': 1840,
    'min_data_in_leaf': 19,
    'feature_fraction': 0.7689008831917789,
    'bagging_fraction': 0.9632405511906635,
    'lambda_l1': 4.3814513363782694,
    'lambda_l2': 1.997171021640737
}

# MODEL NO 1 --> Mean R2: 0.8588071911618131 Std: 0.006513899183962883
model_lgb = LGBMRegressor(**params, random_state=42)
model_lgb.fit(X_train, y_train)
y_pred = model_lgb.predict(X_test)


kf = KFold(n_splits=5, shuffle=True, random_state=42)
scores = cross_val_score(model_lgb, X, y, cv=kf, scoring='r2',verbose=0)
print("Mean R2:", scores.mean(), "Std:", scores.std())

import optuna

def objective(trial):
    param = {
        "objective": "regression",
        "metric": "r2",
        "boosting_type": "gbdt",
        "verbosity": -1,
        "num_leaves": trial.suggest_int("num_leaves", 20, 3000),
        "max_depth": trial.suggest_int("max_depth", 3, 15),
        "learning_rate": trial.suggest_float("learning_rate", 1e-3, 0.3, log=True),
        "n_estimators": trial.suggest_int("n_estimators", 100, 3000),
        "min_data_in_leaf": trial.suggest_int("min_data_in_leaf", 10, 200),
        "feature_fraction": trial.suggest_float("feature_fraction", 0.5, 1.0),
        "bagging_fraction": trial.suggest_float("bagging_fraction", 0.5, 1.0),
        "lambda_l1": trial.suggest_float("lambda_l1", 0.0, 10.0),
        "lambda_l2": trial.suggest_float("lambda_l2", 0.0, 10.0),
    }
    model = LGBMRegressor(**param)

    cv = KFold(n_splits=5, shuffle=True, random_state=42)
    scores = cross_val_score(model, X, y, cv=cv, scoring="r2", n_jobs=-1)

    return np.mean(scores)

# Run Optuna study
study = optuna.create_study(direction="maximize")
study.optimize(objective, n_trials=50)

# Best hyperparameters
print("Best Trial:")
print(study.best_trial.params)
print("Best R2 Score:", study.best_trial.value)

"""# ***MODEL 2***"""

from sklearn.ensemble import HistGradientBoostingRegressor

# MODEL NO 2 --> Mean R2: 0.8361070213231983 Std: 0.00794464669893534
model_2 = HistGradientBoostingRegressor(max_iter=100, learning_rate=0.1, random_state=42)
model_2.fit(X_train, y_train)
y_pred = model_2.predict(X_test)

kf = KFold(n_splits=5, shuffle=True, random_state=42)
scores = cross_val_score(model_2, X, y, cv=kf, scoring='r2',verbose=0)
print("Mean R2:", scores.mean(), "Std:", scores.std())

# IMPROVE VERSION

params = {
 'learning_rate': 0.06553082298395897,
 'max_depth': 12,
 'max_iter': 1160,
 'min_samples_leaf': 90,
 'max_bins': 228,
 'l2_regularization': 9.963895529988566
}


# MODEL NO 2 --> Mean R2: 0.8468154176636183 Std: 0.007875008689564368
model_hgbr = HistGradientBoostingRegressor(**params, random_state=42)
model_hgbr.fit(X_train, y_train)
y_pred = model_hgbr.predict(X_test)


kf = KFold(n_splits=5, shuffle=True, random_state=42)
scores = cross_val_score(model_hgbr, X, y, cv=kf, scoring='r2',verbose=0)
print("Mean R2:", scores.mean(), "Std:", scores.std())

from sklearn.experimental import enable_hist_gradient_boosting  # noqa


def objective(trial):
    param = {
        "learning_rate": trial.suggest_float("learning_rate", 1e-3, 0.3, log=True),
        "max_depth": trial.suggest_int("max_depth", 3, 15),
        "max_iter": trial.suggest_int("max_iter", 100, 3000),
        "min_samples_leaf": trial.suggest_int("min_samples_leaf", 10, 200),
        "max_bins": trial.suggest_int("max_bins", 50, 255),
        "l2_regularization": trial.suggest_float("l2_regularization", 0.0, 10.0),
    }

    model = HistGradientBoostingRegressor(
        random_state=42,
        **param
    )

    cv = KFold(n_splits=5, shuffle=True, random_state=42)
    scores = cross_val_score(model, X, y, cv=cv, scoring="r2", n_jobs=-1)

    return np.mean(scores)

# Run Optuna study
study = optuna.create_study(direction="maximize")
study.optimize(objective, n_trials=50)

print("Best Trial:")
print(study.best_trial.params)
print("Best R2 Score:", study.best_trial.value)

"""# ***MODEL NO 3***"""

from xgboost import XGBRegressor


# MODEL NO 3 --> Mean R2: 0.8323809735739877 Std: 0.008825664795818419
model_3 = XGBRegressor(n_estimators=100, learning_rate=0.1, random_state=42)
model_3.fit(X_train, y_train)
y_pred = model_3.predict(X_test)

kf = KFold(n_splits=5, shuffle=True, random_state=42)
scores = cross_val_score(model_3, X, y, cv=kf, scoring='r2',verbose=0)
print("Mean R2:", scores.mean(), "Std:", scores.std())

# IMPROVE VERSION

params = {'n_estimators': 2380,
          'max_depth': 14,
          'learning_rate': 0.024879839743713116,
          'subsample': 0.8339628646959569,
          'colsample_bytree': 0.5563815314615177,
          'gamma': 0.01651901319424024,
          'reg_alpha': 6.32412247182362,
          'reg_lambda': 6.821421650137928,
          'min_child_weight': 8
}

# MODEL NO 3 --> Mean R2: 0.8572076624008501 Std: 0.006698271164797762
model_xgb = XGBRegressor(**params, random_state=42)
model_xgb.fit(X_train, y_train)
y_pred = model_xgb.predict(X_test)


kf = KFold(n_splits=5, shuffle=True, random_state=42)
scores = cross_val_score(model_xgb, X, y, cv=kf, scoring='r2',verbose=0)
print("Mean R2:", scores.mean(), "Std:", scores.std())

import optuna
import numpy as np
from sklearn.model_selection import KFold, cross_val_score
from xgboost import XGBRegressor

def objective(trial):
    param = {
        "n_estimators": trial.suggest_int("n_estimators", 100, 3000),  # boosting rounds
        "max_depth": trial.suggest_int("max_depth", 3, 15),           # tree depth
        "learning_rate": trial.suggest_float("learning_rate", 1e-3, 0.3, log=True),  # eta
        "subsample": trial.suggest_float("subsample", 0.5, 1.0),      # row sampling
        "colsample_bytree": trial.suggest_float("colsample_bytree", 0.5, 1.0),  # feature sampling
        "gamma": trial.suggest_float("gamma", 0.0, 10.0),             # min loss reduction
        "reg_alpha": trial.suggest_float("reg_alpha", 0.0, 10.0),     # L1 regularization
        "reg_lambda": trial.suggest_float("reg_lambda", 0.0, 10.0),   # L2 regularization
        "min_child_weight": trial.suggest_int("min_child_weight", 1, 20),  # min sum of instance weight
    }

    model = XGBRegressor(
        random_state=42,
        tree_method="hist",  # faster histogram method (GPU if available)
        n_jobs=-1,
        **param
    )

    cv = KFold(n_splits=5, shuffle=True, random_state=42)
    scores = cross_val_score(model, X, y, cv=cv, scoring="r2", n_jobs=-1)

    return np.mean(scores)

# Run Optuna study
study = optuna.create_study(direction="maximize")
study.optimize(objective, n_trials=50)

print("Best Trial:")
print(study.best_trial.params)
print("Best R2 Score:", study.best_trial.value)

"""# ***MODEL NO 4***"""

from sklearn.ensemble import ExtraTreesRegressor

# MODEL NO 4 --> Mean R2: 0.8129821482948131 Std: 0.008107430535175927
model_4 = ExtraTreesRegressor(n_estimators=100, random_state=42)
model_4.fit(X_train, y_train)
y_pred = model_4.predict(X_test)


kf = KFold(n_splits=5, shuffle=True, random_state=42)
scores = cross_val_score(model_4, X, y, cv=kf, scoring='r2',verbose=0)
print("Mean R2:", scores.mean(), "Std:", scores.std())

# IMPROVE VERSION --> bad perform than base model

from sklearn.ensemble import ExtraTreesRegressor

params = {'n_estimators': 808,
          'max_depth': 25,
          'min_samples_split': 9,
          'min_samples_leaf': 1,
          'max_features': 'log2',
          'bootstrap': True
}


# MODEL NO 4 --> Mean R2: 0.8045564140167152 Std: 0.008916593048545736
model_etr = ExtraTreesRegressor(**params, random_state=42)
model_etr.fit(X_train, y_train)
y_pred = model_etr.predict(X_test)


kf = KFold(n_splits=5, shuffle=True, random_state=42)
scores = cross_val_score(model_etr, X, y, cv=kf, scoring='r2',verbose=0)
print("Mean R2:", scores.mean(), "Std:", scores.std())

def objective(trial):
    param = {
        "n_estimators": trial.suggest_int("n_estimators", 100, 2000),  # number of trees
        "max_depth": trial.suggest_int("max_depth", 5, 50),            # tree depth
        "min_samples_split": trial.suggest_int("min_samples_split", 2, 20),
        "min_samples_leaf": trial.suggest_int("min_samples_leaf", 1, 20),
        "max_features": trial.suggest_categorical("max_features", ["sqrt", "log2", None]),
        "bootstrap": trial.suggest_categorical("bootstrap", [True, False]),
    }

    model = ExtraTreesRegressor(
        random_state=42,
        n_jobs=-1,
        **param
    )

    cv = KFold(n_splits=5, shuffle=True, random_state=42)
    scores = cross_val_score(model, X, y, cv=cv, scoring="r2", n_jobs=-1)

    return np.mean(scores)

# Run Optuna study
study = optuna.create_study(direction="maximize")
study.optimize(objective, n_trials=50)

print("Best Trial:")
print(study.best_trial.params)
print("Best R2 Score:", study.best_trial.value)

"""# ***PREDICTION***"""

# Example: take the first 5 rows of X_test as raw input
raw_input_demo = X_test.head(5)
print("Raw input data:")
print(raw_input_demo)

# Predict using the reusable pipeline
y_pred_demo = model_lgb.predict(raw_input_demo)

print("\nPredicted target values:")
print(y_pred_demo)

X_test.iloc[0:5]

y_test.iloc[0:5]

"""# ***saving model***"""

import joblib
# Save
joblib.dump(model_lgb, "lgbm_model.pkl")

"""# ***LOAD AND PREDICT***"""

# Load
loaded_model = joblib.load("lgbm_model.pkl")

X_test.head(5)

# Predict
loaded_model.predict(X_test.head(5))

"""# ***FRONTEND***"""

!pip install streamlit

import streamlit as st
import numpy as np

st.title("California House Price Predictor App")

st.write("Set each feature value individually, then click *Predict*")

# Define sliders individually (different ranges, defaults, steps)
f1 = st.slider("MedInc", min_value=-1, max_value=150, value=2, step=1)
f2 = st.slider("House Age", min_value=1, max_value=50, value=5, step=1)
f3 = st.slider("No Of Rooms ", min_value=1, max_value=15, value=20, step=2)
f4 = st.slider("No Of Bedrooms", min_value=-1, max_value=5, value=0, step=5)
f5 = st.slider("neighborhood size", min_value=0, max_value=5000, value=100, step=10)
f6 = st.slider("Family Members", min_value=1, max_value=10, value=0, step=1)  # like binary
f7 = st.slider("Latitudes", min_value=32, max_value=42, value=0.5, step=0.01)
f8 = st.slider("Longitude", min_value=-125, max_value=-114, value=75, step=5)

# Collect values
features = np.array([f1, f2, f3, f4, f5, f6, f7, f8])

# Predict button
if st.button("Predict"):
    # Example: just return sum of all features
    prediction = np.sum(features)
    st.success(f"Prediction result: {prediction}")

"""# ***FINAL PROJECT***"""

!pip install streamlit pyngrok -q

# Write your Streamlit app into a file
with open("app.py", "w") as f:
    f.write("""
import streamlit as st
import numpy as np
import joblib

# Load model
loaded_model = joblib.load("lgbm_model.pkl")

st.set_page_config(page_title="California House Price Predictor", layout="wide")

st.title("\t\t\t\t🏡 California House Price Predictor App")

# Two-column layout
col1, col2 = st.columns([1, 2])

# --- Left Column: Input sliders ---
with col1:
    st.subheader("🔧 Input Features")
    f1 = st.slider("Median Income (10k$)", 0.5, 15.0, 3.5, step=0.1)
    f2 = st.slider("House Age (years)", 1, 52, 29, step=1)
    f3 = st.slider("No of Rooms", 1.0, 15.0, 5.0, step=1)
    f4 = st.slider("No of Bedrooms", 0.5, 5.0, 1.0, step=1)
    f5 = st.slider("Neighborhood Population", 0, 5000, 1000, step=50)
    f6 = st.slider("Average Occupancy (family size)", 1.0, 10.0, 3.0, step=0.1)
    f7 = st.slider("Latitude", 32.0, 42.0, 35.0, step=0.01)
    f8 = st.slider("Longitude", -125.0, -114.0, -119.0, step=0.01)

    features = np.array([[f1, f2, f3, f4, f5, f6, f7, f8]])

    if st.button("🔮 Predict"):
        prediction = loaded_model.predict(features)[0]
        st.session_state.pred = prediction

# --- Right Column: Prediction Results ---
with col2:
    st.subheader("📊 Prediction Results")

    if "pred" in st.session_state:
        prediction = st.session_state.pred
        st.success(f"🏠 Estimated Median House Value: **${prediction * 100000:,.2f}**")

        # Show house image
        st.image("/content/moiz1.jpg", width=300)

        # Fun animations
        st.balloons()
        st.progress(min(prediction / 5, 1.0))  # 5 is max MedHouseVal in dataset
    else:
        st.info("👉 Adjust the sliders and click **Predict** to see results here!")
""")

# Expose Streamlit app via ngrok
from pyngrok import ngrok
import subprocess

# Set your token
ngrok.set_auth_token("31v6TD51eSw9bKdFPm2XFtlSnjE_2j4oMvVGiSM2KzWw267t7")

# Kill old tunnels
ngrok.kill()

# Start a new tunnel
public_url = ngrok.connect(8501)
print("👉 Open the app here:", public_url)

# Launch Streamlit
process = subprocess.Popen(["streamlit", "run", "app.py"])

pip install -r requirement.txt

X_test.tail(5)

y_test.tail()

